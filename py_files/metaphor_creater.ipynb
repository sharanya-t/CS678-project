{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "\n",
    "word = \"departure\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "source": [
    "with open('saved_objects/words/top_words.txt') as tf:\n",
    "    top_common_words = [line.strip() for line in tf.readlines()]\n",
    "print(len(top_common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Noun': ['the act of departing', 'a variation that deviates from the standard or norm', 'euphemistic expressions for death']}\n"
     ]
    }
   ],
   "source": [
    "print (dictionary.meaning(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " departing variation norm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "d = dictionary.meaning(word)\n",
    "#summary = word\n",
    "summary = \"\"\n",
    "num = 2\n",
    "for pos in d:\n",
    "    for a in d[pos][:min(num, len(d[pos]))]:  # update for more pos's? maybe try with 2 not 3 definitions\n",
    "        num = max(1,num-1)\n",
    "        words = a.split(\" \")\n",
    "        i = 0\n",
    "        while len(words[i]) <= 3 or words[i] in [word] or words[i] in summary: i += 1\n",
    "        summary += \" \" + words[i]\n",
    "        j = 1\n",
    "        while j < len(words) and len(words[-j]) <= 3 or words[-j] in [word] or words[-j] in summary: j += 1\n",
    "        if j != len(words): summary += \" \" + words[-j]\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the act of depart a variation that deviates from the standard or norm euphemistic expressions for death \n"
     ]
    }
   ],
   "source": [
    "meaning = \"\"\n",
    "for pos in d:\n",
    "    for sent in d[pos]:\n",
    "        meaning += stemmer.stem(sent) + \" \"\n",
    "print(meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['departing', 'variation', 'norm']\n",
      "torch.Size([5, 30522])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokens = tokenizer.tokenize(summary)\n",
    "print(tokens)\n",
    "input_ids = torch.tensor(tokenizer.encode(tokens)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "predictions = outputs[0][0]\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def is_good_word(stri):\n",
    "    print(stri)\n",
    "    punctuation = ['-', ':', '+', ',', '～', '`', '!', '.', ',', '?', '*', \"\\\"\", \"|\", \"।\", \"॥\"]\n",
    "    if stri in punctuation or '#' in stri:\n",
    "        return False\n",
    "    if stri == word or stri in summary:\n",
    "        return False\n",
    "    if stemmer.stem(stri) in meaning:\n",
    "        return False\n",
    "    if stri in top_common_words:\n",
    "        return False\n",
    "    pos = nltk.pos_tag([stri])[0][1]\n",
    "    if 'JJ' in pos or 'JJ' in pos: # or 'RB' in pos or 'VB' in pos: #if its a noun, adjective(? or adverb we good just noun for now\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522])\n",
      "the\n",
      ".\n",
      ",\n",
      "norm\n",
      "a\n",
      "-\n",
      "in\n",
      "##s\n",
      ")\n",
      "residual\n",
      "tensor([21961])\n",
      "The metaphor is residual\n"
     ]
    }
   ],
   "source": [
    "#with torch.no_grad():\n",
    "#average = torch.mean(predictions[1:-1], 0) ##improve this!!!\n",
    "average = torch.median(predictions[1:-1], 0).values\n",
    "#average = torch.max(predictions[1:-1], 0).values\n",
    "print(average.shape)\n",
    "x = sorted(average)\n",
    "k = 1\n",
    "best = torch.argmax(average)\n",
    "while not is_good_word(tokenizer.convert_ids_to_tokens(best.item())):\n",
    "    k+=1\n",
    "    y = x[-k]\n",
    "    best = (average == y).nonzero().flatten()\n",
    "print(best)\n",
    "print(\"The metaphor is\", tokenizer.convert_ids_to_tokens(best.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the act of depart a variation that deviates from the standard or norm euphemistic expressions for death \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'depatur'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.convert_ids_to_tokens(nltk.pos_tag([1024])[0][1]) == ':'\n",
    "#nltk.pos_tag([tokenizer.convert_ids_to_tokens(1024)])[0][1] == ':'\n",
    "print(meaning)\n",
    "stemmer.stem('depature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['kappa',\n",
       " '##hel',\n",
       " '80s',\n",
       " 'strengthening',\n",
       " 'appealing',\n",
       " 'brewing',\n",
       " 'gypsy',\n",
       " 'mali',\n",
       " 'lashes',\n",
       " 'hulk',\n",
       " 'unpleasant',\n",
       " 'harassment',\n",
       " 'bio',\n",
       " 'treaties',\n",
       " 'predict',\n",
       " 'instrumentation',\n",
       " 'pulp',\n",
       " 'troupe',\n",
       " 'boiling',\n",
       " 'mantle',\n",
       " '##ffe',\n",
       " 'ins',\n",
       " '##vn',\n",
       " 'dividing',\n",
       " 'handles',\n",
       " 'verbs',\n",
       " '##onal',\n",
       " 'coconut',\n",
       " 'senegal',\n",
       " '340',\n",
       " 'thorough',\n",
       " 'gum',\n",
       " 'momentarily',\n",
       " '##sto',\n",
       " 'cocaine',\n",
       " 'panicked',\n",
       " 'destined',\n",
       " '##turing',\n",
       " 'teatro',\n",
       " 'denying',\n",
       " 'weary',\n",
       " 'captained',\n",
       " 'mans',\n",
       " '##hawks',\n",
       " '##code',\n",
       " 'wakefield',\n",
       " 'bollywood',\n",
       " 'thankfully',\n",
       " '##16',\n",
       " 'cyril',\n",
       " '##wu',\n",
       " 'amendments',\n",
       " '##bahn',\n",
       " 'consultation',\n",
       " 'stud',\n",
       " 'reflections',\n",
       " 'kindness',\n",
       " '1787',\n",
       " 'internally',\n",
       " '##ovo',\n",
       " 'tex',\n",
       " 'mosaic',\n",
       " 'distribute',\n",
       " 'paddy',\n",
       " 'seeming',\n",
       " '143',\n",
       " '##hic',\n",
       " 'piers',\n",
       " '##15',\n",
       " '##mura',\n",
       " '##verse',\n",
       " 'popularly',\n",
       " 'winger',\n",
       " 'kang',\n",
       " 'sentinel',\n",
       " 'mccoy',\n",
       " '##anza',\n",
       " 'covenant',\n",
       " '##bag',\n",
       " 'verge',\n",
       " 'fireworks',\n",
       " 'suppress',\n",
       " 'thrilled',\n",
       " 'dominate',\n",
       " '##jar',\n",
       " 'swansea',\n",
       " '##60',\n",
       " '142',\n",
       " 'reconciliation',\n",
       " '##ndi',\n",
       " 'stiffened',\n",
       " 'cue',\n",
       " 'dorian',\n",
       " '##uf',\n",
       " 'damascus',\n",
       " 'amor',\n",
       " 'ida',\n",
       " 'foremost',\n",
       " '##aga',\n",
       " 'porsche',\n",
       " 'unseen',\n",
       " 'dir',\n",
       " '##had',\n",
       " '##azi',\n",
       " 'stony',\n",
       " 'lexi',\n",
       " 'melodies',\n",
       " '##nko',\n",
       " 'angular',\n",
       " 'integer',\n",
       " 'podcast',\n",
       " 'ants',\n",
       " 'inherent',\n",
       " 'jaws',\n",
       " 'justify',\n",
       " 'persona',\n",
       " '##olved',\n",
       " 'josephine',\n",
       " '##nr',\n",
       " '##ressed',\n",
       " 'customary',\n",
       " 'flashes',\n",
       " 'gala',\n",
       " 'cyrus',\n",
       " 'glaring',\n",
       " 'backyard',\n",
       " 'ariel',\n",
       " 'physiology',\n",
       " 'greenland',\n",
       " 'html',\n",
       " 'stir',\n",
       " 'avon',\n",
       " 'atletico',\n",
       " 'finch',\n",
       " 'methodology',\n",
       " 'ked',\n",
       " '##lent',\n",
       " 'mas',\n",
       " 'catholicism',\n",
       " 'townsend',\n",
       " 'branding',\n",
       " 'quincy',\n",
       " 'fits',\n",
       " 'containers',\n",
       " '1777',\n",
       " 'ashore',\n",
       " 'aragon',\n",
       " '##19',\n",
       " 'forearm',\n",
       " 'poisoning',\n",
       " '##sd',\n",
       " 'adopting',\n",
       " 'conquer',\n",
       " 'grinding',\n",
       " 'amnesty',\n",
       " 'keller',\n",
       " 'finances',\n",
       " 'evaluate',\n",
       " 'forged',\n",
       " 'lankan',\n",
       " 'instincts',\n",
       " '##uto',\n",
       " 'guam',\n",
       " 'bosnian',\n",
       " 'photographed',\n",
       " 'workplace',\n",
       " 'desirable',\n",
       " 'protector',\n",
       " '##dog',\n",
       " 'allocation',\n",
       " 'intently',\n",
       " 'encourages',\n",
       " 'willy',\n",
       " '##sten',\n",
       " 'bodyguard',\n",
       " 'electro',\n",
       " 'brighter',\n",
       " '##ν',\n",
       " 'bihar',\n",
       " '##chev',\n",
       " 'lasts',\n",
       " 'opener',\n",
       " 'amphibious',\n",
       " 'sal',\n",
       " 'verde',\n",
       " 'arte',\n",
       " '##cope',\n",
       " 'captivity',\n",
       " 'vocabulary',\n",
       " 'yields',\n",
       " '##tted',\n",
       " 'agreeing',\n",
       " 'desmond',\n",
       " 'pioneered',\n",
       " '##chus',\n",
       " 'strap',\n",
       " 'campaigned',\n",
       " 'railroads',\n",
       " '##ович',\n",
       " 'emblem',\n",
       " '##dre',\n",
       " 'stormed',\n",
       " '501',\n",
       " '##ulous',\n",
       " 'marijuana',\n",
       " 'northumberland',\n",
       " '##gn',\n",
       " '##nath',\n",
       " 'bowen',\n",
       " 'landmarks',\n",
       " 'beaumont',\n",
       " '##qua',\n",
       " 'danube',\n",
       " '##bler',\n",
       " 'attorneys',\n",
       " 'th',\n",
       " 'ge',\n",
       " 'flyers',\n",
       " 'critique',\n",
       " 'villains',\n",
       " 'cass',\n",
       " 'mutation',\n",
       " 'acc',\n",
       " '##0s',\n",
       " 'colombo',\n",
       " 'mckay',\n",
       " 'motif',\n",
       " 'sampling',\n",
       " 'concluding',\n",
       " 'syndicate',\n",
       " '##rell',\n",
       " 'neon',\n",
       " 'stables',\n",
       " 'ds',\n",
       " 'warnings',\n",
       " 'clint',\n",
       " 'mourning',\n",
       " 'wilkinson',\n",
       " '##tated',\n",
       " 'merrill',\n",
       " 'leopard',\n",
       " 'evenings',\n",
       " 'exhaled',\n",
       " 'emil',\n",
       " 'sonia',\n",
       " 'ezra',\n",
       " 'discrete',\n",
       " 'stove',\n",
       " 'farrell',\n",
       " 'fifteenth',\n",
       " 'prescribed',\n",
       " 'superhero',\n",
       " '##rier',\n",
       " 'worms',\n",
       " 'helm',\n",
       " 'wren',\n",
       " '##duction',\n",
       " '##hc',\n",
       " 'expo',\n",
       " '##rator',\n",
       " 'hq',\n",
       " 'unfamiliar',\n",
       " 'antony',\n",
       " 'prevents',\n",
       " 'acceleration',\n",
       " 'fiercely',\n",
       " 'mari',\n",
       " 'painfully',\n",
       " 'calculations',\n",
       " 'cheaper',\n",
       " 'ign',\n",
       " 'clifton',\n",
       " 'irvine',\n",
       " 'davenport',\n",
       " 'mozambique',\n",
       " '##np',\n",
       " 'pierced',\n",
       " '##evich',\n",
       " 'wonders',\n",
       " '##wig',\n",
       " '##cate',\n",
       " '##iling',\n",
       " 'crusade',\n",
       " 'ware',\n",
       " '##uel',\n",
       " 'enzymes',\n",
       " 'reasonably',\n",
       " 'mls',\n",
       " '##coe',\n",
       " 'mater',\n",
       " 'ambition',\n",
       " 'bunny',\n",
       " 'eliot',\n",
       " 'kernel',\n",
       " '##fin',\n",
       " 'asphalt',\n",
       " 'headmaster',\n",
       " 'torah',\n",
       " 'aden',\n",
       " 'lush',\n",
       " 'pins',\n",
       " 'waived',\n",
       " '##care',\n",
       " '##yas',\n",
       " 'joao',\n",
       " 'substrate',\n",
       " 'enforce',\n",
       " '##grad',\n",
       " '##ules',\n",
       " 'alvarez',\n",
       " 'selections',\n",
       " 'epidemic',\n",
       " 'tempted',\n",
       " '##bit',\n",
       " 'bremen',\n",
       " 'translates',\n",
       " 'ensured',\n",
       " 'waterfront',\n",
       " '29th',\n",
       " 'forrest',\n",
       " 'manny',\n",
       " 'malone',\n",
       " 'kramer',\n",
       " 'reigning',\n",
       " 'cookies',\n",
       " 'simpler',\n",
       " 'absorption',\n",
       " '205',\n",
       " 'engraved',\n",
       " '##ffy',\n",
       " 'evaluated',\n",
       " '1778',\n",
       " 'haze',\n",
       " '146',\n",
       " 'comforting',\n",
       " 'crossover',\n",
       " '##abe',\n",
       " 'thorn',\n",
       " '##rift',\n",
       " '##imo',\n",
       " '##pop',\n",
       " 'suppression',\n",
       " 'fatigue',\n",
       " 'cutter',\n",
       " '##tr',\n",
       " '201',\n",
       " 'wurttemberg',\n",
       " '##orf',\n",
       " 'enforced',\n",
       " 'hovering',\n",
       " 'proprietary',\n",
       " 'gb',\n",
       " 'samurai',\n",
       " 'syllable',\n",
       " 'ascent',\n",
       " 'lacey',\n",
       " 'tick',\n",
       " 'lars',\n",
       " 'tractor',\n",
       " 'merchandise',\n",
       " 'rep',\n",
       " 'bouncing',\n",
       " 'defendants',\n",
       " '##yre',\n",
       " 'huntington',\n",
       " '##ground',\n",
       " '##oko',\n",
       " 'standardized',\n",
       " '##hor',\n",
       " '##hima',\n",
       " 'assassinated',\n",
       " 'nu',\n",
       " 'predecessors',\n",
       " 'rainy',\n",
       " 'liar',\n",
       " 'assurance',\n",
       " 'lyrical',\n",
       " '##uga',\n",
       " 'secondly',\n",
       " 'flattened',\n",
       " 'ios',\n",
       " 'parameter',\n",
       " 'undercover',\n",
       " '##mity',\n",
       " 'bordeaux',\n",
       " 'punish',\n",
       " 'ridges',\n",
       " 'markers',\n",
       " 'exodus',\n",
       " 'inactive',\n",
       " 'hesitate',\n",
       " 'debbie',\n",
       " 'nyc',\n",
       " 'pledge',\n",
       " 'savoy',\n",
       " 'nagar',\n",
       " 'offset',\n",
       " 'organist',\n",
       " '##tium',\n",
       " 'hesse',\n",
       " 'marin',\n",
       " 'converting',\n",
       " '##iver',\n",
       " 'diagram',\n",
       " 'propulsion',\n",
       " 'pu',\n",
       " 'validity',\n",
       " 'reverted',\n",
       " 'supportive',\n",
       " '##dc',\n",
       " 'ministries',\n",
       " 'clans',\n",
       " 'responds',\n",
       " 'proclamation',\n",
       " '##inae',\n",
       " '##ø',\n",
       " '##rea',\n",
       " 'ein',\n",
       " 'pleading',\n",
       " 'patriot',\n",
       " 'sf',\n",
       " 'birch',\n",
       " 'islanders',\n",
       " 'strauss',\n",
       " 'hates',\n",
       " '##dh',\n",
       " 'brandenburg',\n",
       " 'concession',\n",
       " 'rd',\n",
       " '##ob',\n",
       " '1900s',\n",
       " 'killings',\n",
       " 'textbook',\n",
       " 'antiquity',\n",
       " 'cinematography',\n",
       " 'wharf',\n",
       " 'embarrassing',\n",
       " 'setup',\n",
       " 'creed',\n",
       " 'farmland',\n",
       " 'inequality',\n",
       " 'centred',\n",
       " 'signatures',\n",
       " 'fallon',\n",
       " '370',\n",
       " '##ingham',\n",
       " '##uts',\n",
       " 'ceylon',\n",
       " 'gazing',\n",
       " 'directive',\n",
       " 'laurie',\n",
       " '##tern',\n",
       " 'globally',\n",
       " '##uated',\n",
       " '##dent',\n",
       " 'allah',\n",
       " 'excavation',\n",
       " 'threads',\n",
       " '##cross',\n",
       " '148',\n",
       " 'frantically',\n",
       " 'icc',\n",
       " 'utilize',\n",
       " 'determines',\n",
       " 'respiratory',\n",
       " 'thoughtful',\n",
       " 'receptions',\n",
       " '##dicate',\n",
       " 'merging',\n",
       " 'chandra',\n",
       " 'seine',\n",
       " '147',\n",
       " 'builders',\n",
       " 'builds',\n",
       " 'diagnostic',\n",
       " 'dev',\n",
       " 'visibility',\n",
       " 'goddamn',\n",
       " 'analyses',\n",
       " 'dhaka',\n",
       " 'cho',\n",
       " 'proves',\n",
       " 'chancel',\n",
       " 'concurrent',\n",
       " 'curiously',\n",
       " 'canadians',\n",
       " 'pumped',\n",
       " 'restoring',\n",
       " '1850s',\n",
       " 'turtles',\n",
       " 'jaguar',\n",
       " 'sinister',\n",
       " 'spinal',\n",
       " 'traction',\n",
       " 'declan',\n",
       " 'vows',\n",
       " '1784',\n",
       " 'glowed',\n",
       " 'capitalism',\n",
       " 'swirling',\n",
       " 'install',\n",
       " 'universidad',\n",
       " '##lder',\n",
       " '##oat',\n",
       " 'soloist',\n",
       " '##genic',\n",
       " '##oor',\n",
       " 'coincidence',\n",
       " 'beginnings',\n",
       " 'nissan',\n",
       " 'dip',\n",
       " 'resorts',\n",
       " 'caucasus',\n",
       " 'combustion',\n",
       " 'infectious',\n",
       " '##eno',\n",
       " 'pigeon',\n",
       " 'serpent',\n",
       " '##itating',\n",
       " 'conclude',\n",
       " 'masked',\n",
       " 'salad',\n",
       " 'jew',\n",
       " '##gr',\n",
       " 'surreal',\n",
       " 'toni',\n",
       " '##wc',\n",
       " 'harmonica',\n",
       " '151',\n",
       " '##gins',\n",
       " '##etic',\n",
       " '##coat',\n",
       " 'fishermen',\n",
       " 'intending',\n",
       " 'bravery',\n",
       " '##wave',\n",
       " 'klaus',\n",
       " 'titan',\n",
       " 'wembley',\n",
       " 'taiwanese',\n",
       " 'ransom',\n",
       " '40th',\n",
       " 'incorrect',\n",
       " 'hussein',\n",
       " 'eyelids',\n",
       " 'jp',\n",
       " 'cooke',\n",
       " 'dramas',\n",
       " 'utilities',\n",
       " '##etta',\n",
       " '##print',\n",
       " 'eisenhower',\n",
       " 'principally',\n",
       " 'granada',\n",
       " 'lana',\n",
       " '##rak',\n",
       " 'openings',\n",
       " 'concord',\n",
       " '##bl',\n",
       " 'bethany',\n",
       " 'connie',\n",
       " 'morality',\n",
       " 'sega',\n",
       " '##mons',\n",
       " '##nard',\n",
       " 'earnings',\n",
       " '##kara',\n",
       " '##cine',\n",
       " 'wii',\n",
       " 'communes',\n",
       " '##rel',\n",
       " 'coma',\n",
       " 'composing',\n",
       " 'softened',\n",
       " 'severed',\n",
       " 'grapes',\n",
       " '##17',\n",
       " 'nguyen',\n",
       " 'analyzed',\n",
       " 'warlord',\n",
       " 'hubbard',\n",
       " 'heavenly',\n",
       " 'behave',\n",
       " 'slovenian',\n",
       " '##hit',\n",
       " '##ony',\n",
       " 'hailed',\n",
       " 'filmmakers',\n",
       " 'trance',\n",
       " 'caldwell',\n",
       " 'skye',\n",
       " 'unrest',\n",
       " 'coward',\n",
       " 'likelihood',\n",
       " '##aging',\n",
       " 'bern',\n",
       " 'sci',\n",
       " 'taliban',\n",
       " 'honolulu',\n",
       " 'propose',\n",
       " '##wang',\n",
       " '1700',\n",
       " 'browser',\n",
       " 'imagining',\n",
       " 'cobra',\n",
       " 'contributes',\n",
       " 'dukes',\n",
       " 'instinctively',\n",
       " 'conan',\n",
       " 'violinist',\n",
       " '##ores',\n",
       " 'accessories',\n",
       " 'gradual',\n",
       " '##amp',\n",
       " 'quotes',\n",
       " 'sioux',\n",
       " '##dating',\n",
       " 'undertake',\n",
       " 'intercepted',\n",
       " 'sparkling',\n",
       " 'compressed',\n",
       " '139',\n",
       " 'fungus',\n",
       " 'tombs',\n",
       " 'haley',\n",
       " 'imposing',\n",
       " 'rests',\n",
       " 'degradation',\n",
       " 'lincolnshire',\n",
       " 'retailers',\n",
       " 'wetlands',\n",
       " 'tulsa',\n",
       " 'distributor',\n",
       " 'dungeon',\n",
       " 'nun',\n",
       " 'greenhouse',\n",
       " 'convey',\n",
       " 'atlantis',\n",
       " 'aft',\n",
       " 'exits',\n",
       " 'oman',\n",
       " 'dresser',\n",
       " 'lyons',\n",
       " '##sti',\n",
       " 'joking',\n",
       " 'eddy',\n",
       " 'judgement',\n",
       " 'omitted',\n",
       " 'digits',\n",
       " '##cts',\n",
       " '##game',\n",
       " 'juniors',\n",
       " '##rae',\n",
       " 'cents',\n",
       " 'stricken',\n",
       " 'une',\n",
       " '##ngo',\n",
       " 'wizards',\n",
       " 'weir',\n",
       " 'breton',\n",
       " 'nan',\n",
       " 'technician',\n",
       " 'fibers',\n",
       " 'liking',\n",
       " 'royalty',\n",
       " '##cca',\n",
       " '154',\n",
       " 'persia',\n",
       " 'terribly',\n",
       " 'magician',\n",
       " '##rable',\n",
       " '##unt',\n",
       " 'vance',\n",
       " 'cafeteria',\n",
       " 'booker',\n",
       " 'camille',\n",
       " 'warmer',\n",
       " '##static',\n",
       " 'consume',\n",
       " 'cavern',\n",
       " 'gaps',\n",
       " 'compass',\n",
       " 'contemporaries',\n",
       " 'foyer',\n",
       " 'soothing',\n",
       " 'graveyard',\n",
       " 'maj',\n",
       " 'plunged',\n",
       " 'blush',\n",
       " '##wear',\n",
       " 'cascade',\n",
       " 'demonstrates',\n",
       " 'ordinance',\n",
       " '##nov',\n",
       " 'boyle',\n",
       " '##lana',\n",
       " 'rockefeller',\n",
       " 'shaken',\n",
       " 'banjo',\n",
       " 'izzy',\n",
       " '##ense',\n",
       " 'breathless',\n",
       " 'vines',\n",
       " '##32',\n",
       " '##eman',\n",
       " 'alterations',\n",
       " 'chromosome',\n",
       " 'dwellings',\n",
       " 'feudal',\n",
       " 'mole',\n",
       " '153',\n",
       " 'catalonia',\n",
       " 'relics',\n",
       " 'tenant',\n",
       " 'mandated',\n",
       " '##fm',\n",
       " 'fridge',\n",
       " 'hats',\n",
       " 'honesty',\n",
       " 'patented',\n",
       " 'raul',\n",
       " 'heap',\n",
       " 'cruisers',\n",
       " 'accusing',\n",
       " 'enlightenment',\n",
       " 'infants',\n",
       " 'wherein',\n",
       " 'chatham',\n",
       " 'contractors',\n",
       " 'zen',\n",
       " 'affinity',\n",
       " 'hc',\n",
       " 'osborne',\n",
       " 'piston',\n",
       " '156',\n",
       " 'traps',\n",
       " 'maturity',\n",
       " '##rana',\n",
       " 'lagos',\n",
       " '##zal',\n",
       " 'peering',\n",
       " '##nay',\n",
       " 'attendant',\n",
       " 'dealers',\n",
       " 'protocols',\n",
       " 'subset',\n",
       " 'prospects',\n",
       " 'biographical',\n",
       " '##cre',\n",
       " 'artery',\n",
       " '##zers',\n",
       " 'insignia',\n",
       " 'nuns',\n",
       " 'endured',\n",
       " '##eration',\n",
       " 'recommend',\n",
       " 'schwartz',\n",
       " 'serbs',\n",
       " 'berger',\n",
       " 'cromwell',\n",
       " 'crossroads',\n",
       " '##ctor',\n",
       " 'enduring',\n",
       " 'clasped',\n",
       " 'grounded',\n",
       " '##bine',\n",
       " 'marseille',\n",
       " 'twitched',\n",
       " 'abel',\n",
       " 'choke',\n",
       " 'https',\n",
       " 'catalyst',\n",
       " 'moldova',\n",
       " 'italians',\n",
       " '##tist',\n",
       " 'disastrous',\n",
       " 'wee',\n",
       " '##oured',\n",
       " '##nti',\n",
       " 'wwf',\n",
       " 'nope',\n",
       " '##piration',\n",
       " '##asa',\n",
       " 'expresses',\n",
       " 'thumbs',\n",
       " '167',\n",
       " '##nza',\n",
       " 'coca',\n",
       " '1781',\n",
       " 'cheating',\n",
       " '##ption',\n",
       " 'skipped',\n",
       " 'sensory',\n",
       " 'heidelberg',\n",
       " 'spies',\n",
       " 'satan',\n",
       " 'dangers',\n",
       " 'semifinal',\n",
       " '202',\n",
       " 'bohemia',\n",
       " 'whitish',\n",
       " 'confusing',\n",
       " 'shipbuilding',\n",
       " 'relies',\n",
       " 'surgeons',\n",
       " 'landings',\n",
       " 'ravi',\n",
       " 'baku',\n",
       " 'moor',\n",
       " 'suffix',\n",
       " 'alejandro',\n",
       " '##yana',\n",
       " 'litre',\n",
       " 'upheld',\n",
       " '##unk',\n",
       " 'rajasthan',\n",
       " '##rek',\n",
       " 'coaster',\n",
       " 'insists',\n",
       " 'posture',\n",
       " 'scenarios',\n",
       " 'etienne',\n",
       " 'favoured',\n",
       " 'appoint',\n",
       " 'transgender',\n",
       " 'elephants',\n",
       " 'poked',\n",
       " 'greenwood',\n",
       " 'defences',\n",
       " 'fulfilled',\n",
       " 'militant',\n",
       " 'somali',\n",
       " '1758',\n",
       " 'chalk',\n",
       " 'potent',\n",
       " '##ucci',\n",
       " 'migrants',\n",
       " 'wink',\n",
       " 'assistants',\n",
       " 'nos',\n",
       " 'restriction',\n",
       " 'activism',\n",
       " 'niger',\n",
       " '##ario',\n",
       " 'colon',\n",
       " 'shaun',\n",
       " '##sat',\n",
       " 'daphne',\n",
       " '##erated',\n",
       " 'swam',\n",
       " 'congregations',\n",
       " 'reprise',\n",
       " 'considerations',\n",
       " 'magnet',\n",
       " 'playable',\n",
       " 'xvi',\n",
       " '##р',\n",
       " 'overthrow',\n",
       " 'tobias',\n",
       " 'knob',\n",
       " 'chavez',\n",
       " 'coding',\n",
       " '##mers',\n",
       " 'propped',\n",
       " 'katrina',\n",
       " 'orient',\n",
       " 'newcomer',\n",
       " '##suke',\n",
       " 'temperate',\n",
       " '##pool',\n",
       " 'farmhouse',\n",
       " 'interrogation',\n",
       " '##vd',\n",
       " 'committing',\n",
       " '##vert',\n",
       " 'forthcoming',\n",
       " 'strawberry',\n",
       " 'joaquin',\n",
       " 'macau',\n",
       " 'ponds',\n",
       " 'shocking',\n",
       " 'siberia',\n",
       " '##cellular',\n",
       " 'chant',\n",
       " 'contributors',\n",
       " '##nant',\n",
       " '##ologists',\n",
       " 'sped',\n",
       " 'absorb',\n",
       " 'hail',\n",
       " '1782',\n",
       " 'spared',\n",
       " '##hore',\n",
       " 'barbados',\n",
       " 'karate',\n",
       " 'opus',\n",
       " 'originates',\n",
       " 'saul',\n",
       " '##xie',\n",
       " 'evergreen',\n",
       " 'leaped',\n",
       " '##rock',\n",
       " 'correlation',\n",
       " 'exaggerated',\n",
       " 'weekday',\n",
       " 'unification',\n",
       " 'bump',\n",
       " 'tracing',\n",
       " 'brig',\n",
       " 'afb',\n",
       " 'pathways',\n",
       " 'utilizing',\n",
       " '##ners',\n",
       " 'mod',\n",
       " 'mb',\n",
       " 'disturbance',\n",
       " 'kneeling',\n",
       " '##stad',\n",
       " '##guchi',\n",
       " '100th',\n",
       " 'pune',\n",
       " '##thy',\n",
       " 'decreasing',\n",
       " '168',\n",
       " 'manipulation',\n",
       " 'miriam',\n",
       " 'academia',\n",
       " 'ecosystem',\n",
       " 'occupational',\n",
       " 'rbi',\n",
       " '##lem',\n",
       " 'rift',\n",
       " '##14',\n",
       " 'rotary',\n",
       " 'stacked',\n",
       " 'incorporation',\n",
       " 'awakening',\n",
       " 'generators',\n",
       " 'guerrero',\n",
       " 'racist',\n",
       " '##omy',\n",
       " 'cyber',\n",
       " 'derivatives',\n",
       " 'culminated',\n",
       " 'allie',\n",
       " 'annals',\n",
       " 'panzer',\n",
       " 'sainte',\n",
       " 'wikipedia',\n",
       " 'pops',\n",
       " 'zu',\n",
       " 'austro',\n",
       " '##vate',\n",
       " 'algerian',\n",
       " 'politely',\n",
       " 'nicholson',\n",
       " 'mornings',\n",
       " 'educate',\n",
       " 'tastes',\n",
       " 'thrill',\n",
       " 'dartmouth',\n",
       " '##gating',\n",
       " 'db',\n",
       " '##jee',\n",
       " 'regan',\n",
       " 'differing',\n",
       " 'concentrating',\n",
       " 'choreography',\n",
       " 'divinity',\n",
       " '##media',\n",
       " 'pledged',\n",
       " 'alexandre',\n",
       " 'routing',\n",
       " 'gregor',\n",
       " 'madeline',\n",
       " '##idal',\n",
       " 'apocalypse',\n",
       " '##hora',\n",
       " 'gunfire',\n",
       " 'culminating',\n",
       " 'elves',\n",
       " 'fined',\n",
       " 'liang',\n",
       " 'lam',\n",
       " 'programmed',\n",
       " 'tar',\n",
       " 'guessing',\n",
       " 'transparency',\n",
       " 'gabrielle',\n",
       " '##gna',\n",
       " 'cancellation',\n",
       " 'flexibility',\n",
       " '##lining',\n",
       " 'accession',\n",
       " 'shea',\n",
       " 'stronghold',\n",
       " 'nets',\n",
       " 'specializes',\n",
       " '##rgan',\n",
       " 'abused',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(list(np.arange(40)))\n",
    "tokenizer.convert_ids_to_tokens(np.arange(16000,18000))\n",
    "#100-103 tags\n",
    "#the (first word) = 1996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quick', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'#' in '##a'\n",
    "nltk.pos_tag([\"quick\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522])\n",
      "[tensor(1012), tensor(1064), tensor(1025), tensor(8386), tensor(1010), tensor(1996), tensor(1011), tensor(1029), tensor(1998), tensor(1999)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.', '|', ';', 'variation', ',', 'the', '-', '?', 'and', 'in']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average = torch.mean(predictions[1:-1], axis = 0)\n",
    "print(average.shape) #####IMPROVE THIS\n",
    "from heapq import nlargest\n",
    "indexes = torch.arange(0,len(average))\n",
    "n = nlargest(10, indexes, key=lambda i: average[i])\n",
    "print(n)\n",
    "tokenizer.convert_ids_to_tokens(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
